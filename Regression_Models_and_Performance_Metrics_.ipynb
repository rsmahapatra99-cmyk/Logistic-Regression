{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **Simple Linear Regression (SLR)** and its purpose:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "**Simple Linear Regression (SLR)** is a **supervised learning technique** used to model the relationship between:\n",
        "\n",
        "* **One independent variable (feature)** (X)\n",
        "* **One dependent variable (target)** (Y)\n",
        "\n",
        "It assumes a **linear relationship** between the two, expressed as:\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y) = dependent variable (what we want to predict)\n",
        "* (X) = independent variable (input feature)\n",
        "* (\\beta_0) = intercept (value of (Y) when (X=0))\n",
        "* (\\beta_1) = slope (change in (Y) for a unit change in (X))\n",
        "* (\\varepsilon) = error term (captures noise or variation not explained by (X))\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Purpose**\n",
        "\n",
        "**Simple Linear Regression is used to:**\n",
        "\n",
        "1. **Predict** the value of a continuous target (Y) based on a single predictor (X).\n",
        "2. **Understand relationships** between two variables:\n",
        "\n",
        "   * Positive slope → (Y) increases as (X) increases\n",
        "   * Negative slope → (Y) decreases as (X) increases\n",
        "3. **Quantify the strength of association** between (X) and (Y) (using R-squared).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Example**\n",
        "\n",
        "Suppose a company wants to predict **sales (Y)** based on **advertising budget (X)**:\n",
        "\n",
        "* Using SLR, we can fit a line:\n",
        "\n",
        "[\n",
        "\\text{Sales} = 50 + 3 \\times \\text{Advertising Budget}\n",
        "]\n",
        "\n",
        "* Interpretation: Every $1 increase in advertising budget is associated with $3 increase in sales.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Key Assumptions**\n",
        "\n",
        "1. **Linearity:** Relationship between (X) and (Y) is linear\n",
        "2. **Independence:** Observations are independent\n",
        "3. **Homoscedasticity:** Constant variance of errors (\\varepsilon)\n",
        "4. **Normality:** Errors are normally distributed\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> **Simple Linear Regression models the linear relationship between a single feature and a continuous target, primarily for prediction and understanding the effect of the feature on the target.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Pea7FHgcdVvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here are the **key assumptions of Simple Linear Regression (SLR)** explained clearly:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Linearity**\n",
        "\n",
        "* The relationship between the independent variable (X) and the dependent variable (Y) is **linear**.\n",
        "* Mathematically: (Y = \\beta_0 + \\beta_1 X + \\varepsilon)\n",
        "* Violation → predictions may be biased or inaccurate.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Independence of Errors**\n",
        "\n",
        "* The residuals (errors) (\\varepsilon_i = Y_i - \\hat{Y}_i) are **independent** of each other.\n",
        "* No correlation between consecutive errors (important in time-series data).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Homoscedasticity (Constant Variance)**\n",
        "\n",
        "* The variance of errors is **constant across all values of (X)**.\n",
        "* Violation → **heteroscedasticity**, which can make confidence intervals and p-values unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Normality of Errors**\n",
        "\n",
        "* The residuals (\\varepsilon) are **normally distributed**.\n",
        "* Important for inference: confidence intervals, hypothesis testing.\n",
        "* Not strictly required for prediction if sample size is large (Central Limit Theorem helps).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. No Multicollinearity**\n",
        "\n",
        "* In **Simple** Linear Regression with only one feature, this is automatically satisfied.\n",
        "* Relevant for **Multiple Linear Regression** when multiple predictors are used.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. No Autocorrelation**\n",
        "\n",
        "* Especially for time-series data, errors should **not be correlated with each other**.\n",
        "* Violation → biased estimates of standard errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Assumption         | Description                     | Why Important                                       |\n",
        "| ------------------ | ------------------------------- | --------------------------------------------------- |\n",
        "| Linearity          | (Y) depends linearly on (X)     | Ensures model correctly represents the relationship |\n",
        "| Independence       | Errors are independent          | Avoids biased standard errors                       |\n",
        "| Homoscedasticity   | Constant error variance         | Reliable hypothesis tests & confidence intervals    |\n",
        "| Normality          | Errors are normally distributed | Needed for valid inference                          |\n",
        "| No Autocorrelation | Errors not correlated           | Avoids misleading significance results              |\n",
        "\n"
      ],
      "metadata": {
        "id": "JGFnDIkvdm6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write the mathematical equation for a simple linear regression model and\n",
        "explain each term.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s the **mathematical equation for a Simple Linear Regression (SLR) model** and a detailed explanation of each term:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Equation**\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Explanation of Each Term**\n",
        "\n",
        "| Term          | Meaning                          | Role in the Model                                                                                   |\n",
        "| ------------- | -------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
        "| (Y)           | Dependent variable / response    | The variable we want to **predict** (e.g., sales, price, exam score)                                |\n",
        "| (X)           | Independent variable / predictor | The input feature used to **explain or predict** (Y) (e.g., advertising spend, years of experience) |\n",
        "| (\\beta_0)     | Intercept                        | Value of (Y) when (X = 0); starting point of the regression line                                    |\n",
        "| (\\beta_1)     | Slope / coefficient              | Change in (Y) for a **unit change in (X)**; measures the strength and direction of the relationship |\n",
        "| (\\varepsilon) | Error term / residual            | Captures **random noise or variability** in (Y) not explained by (X)                                |\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Intuition**\n",
        "\n",
        "* The model fits a **straight line** through the data points.\n",
        "* (\\beta_0) moves the line up/down, (\\beta_1) tilts the line.\n",
        "* The **goal** of regression is to find (\\beta_0) and (\\beta_1) that **minimize the difference** between the predicted (Y) ((\\hat{Y} = \\beta_0 + \\beta_1 X)) and the actual (Y) values, usually using **Least Squares**:\n",
        "\n",
        "[\n",
        "\\text{Minimize } \\sum_{i=1}^{n} (Y_i - (\\beta_0 + \\beta_1 X_i))^2\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "26Ipnd3Ud3b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a **real-world example** where **Simple Linear Regression (SLR)** can be applied:\n",
        "\n",
        "---\n",
        "\n",
        "## **Example: Predicting House Prices Based on Size**\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "* A real estate company wants to **predict the selling price of a house** based on its **area in square feet**.\n",
        "* Here:\n",
        "\n",
        "  * **Independent variable (X):** House size in square feet\n",
        "  * **Dependent variable (Y):** House selling price in dollars\n",
        "\n",
        "**SLR Model Equation:**\n",
        "\n",
        "[\n",
        "\\text{Price} = \\beta_0 + \\beta_1 \\cdot \\text{Size} + \\varepsilon\n",
        "]\n",
        "\n",
        "* (\\beta_0) → Base price of a house (even if size = 0, could represent land/amenities)\n",
        "* (\\beta_1) → How much the price increases per additional square foot\n",
        "* (\\varepsilon) → Random factors affecting price (location, condition, market trends)\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "1. **Predict prices** of new houses for buyers/sellers.\n",
        "2. **Understand the relationship** between house size and price.\n",
        "3. **Inform business decisions** like pricing strategies or investment planning.\n",
        "\n",
        "**Visualization:**\n",
        "\n",
        "* Plot house size on X-axis and price on Y-axis.\n",
        "* Fit a **straight regression line** that best captures the trend.\n",
        "* Predictions can be made by plugging new house sizes into the line.\n",
        "\n"
      ],
      "metadata": {
        "id": "wdFcJ4_leFpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the method of least squares in linear regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of the **Method of Least Squares** in **linear regression**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "The **Method of Least Squares** is a mathematical approach used to **estimate the parameters** ((\\beta_0) and (\\beta_1)) of a linear regression model by **minimizing the sum of squared differences** between the observed values and the predicted values.\n",
        "\n",
        "In simple linear regression:\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "]\n",
        "\n",
        "* (Y_i) = actual observed value\n",
        "* (\\hat{Y}_i = \\beta_0 + \\beta_1 X_i) = predicted value\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Objective**\n",
        "\n",
        "Minimize the **sum of squared errors (residuals)**:\n",
        "\n",
        "[\n",
        "\\text{SSE} = \\sum_{i=1}^{n} (Y_i - \\hat{Y}*i)^2 = \\sum*{i=1}^{n} \\big(Y_i - (\\beta_0 + \\beta_1 X_i)\\big)^2\n",
        "]\n",
        "\n",
        "* Squaring ensures all errors are positive and **penalizes large deviations more**.\n",
        "* The best-fitting line is the one where **SSE is smallest**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. How It Works**\n",
        "\n",
        "1. Start with the regression line: (\\hat{Y} = \\beta_0 + \\beta_1 X)\n",
        "2. Compute residuals for each observation: (e_i = Y_i - \\hat{Y}_i)\n",
        "3. Square each residual: (e_i^2)\n",
        "4. Sum all squared residuals to get SSE\n",
        "5. Find (\\beta_0) and (\\beta_1) that **minimize SSE**\n",
        "\n",
        "   * Using calculus (partial derivatives w.r.t (\\beta_0) and (\\beta_1))\n",
        "\n",
        "**Formulas for SLR coefficients:**\n",
        "\n",
        "[\n",
        "\\beta_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}\n",
        "]\n",
        "\n",
        "[\n",
        "\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}\n",
        "]\n",
        "\n",
        "Where (\\bar{X}) and (\\bar{Y}) are the means of X and Y.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Intuition**\n",
        "\n",
        "* You want the line to be **as close as possible to all data points**.\n",
        "* Squared errors prevent positive and negative deviations from canceling each other out.\n",
        "* This ensures the line represents the **overall trend** of the data.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Summary**\n",
        "\n",
        "> **The method of least squares finds the regression line that minimizes the sum of the squared differences between the actual and predicted values, giving the best linear approximation of the relationship between X and Y.**\n"
      ],
      "metadata": {
        "id": "CnmnHGM2eteI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a detailed explanation of **Logistic Regression** and how it differs from **Linear Regression**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is Logistic Regression?**\n",
        "\n",
        "**Logistic Regression** is a **supervised learning algorithm** used for **classification tasks**.\n",
        "\n",
        "* It predicts the **probability** that a sample belongs to a particular class.\n",
        "* Typically used for **binary classification** (e.g., yes/no, 0/1, spam/not spam).\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "[\n",
        "P(Y=1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n",
        "]\n",
        "\n",
        "* The **sigmoid (logistic) function** maps any real-valued input to a value between 0 and 1.\n",
        "* Thresholding (commonly 0.5) is used to assign the final class:\n",
        "\n",
        "  * (P \\ge 0.5 \\rightarrow Y=1)\n",
        "  * (P < 0.5 \\rightarrow Y=0)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Key Differences from Linear Regression**\n",
        "\n",
        "| Aspect                     | Linear Regression                                      | Logistic Regression                                                      |\n",
        "| -------------------------- | ------------------------------------------------------ | ------------------------------------------------------------------------ |\n",
        "| **Purpose**                | Predict a **continuous outcome** (e.g., price, height) | Predict a **categorical outcome** (usually 0/1)                          |\n",
        "| **Output**                 | Real number ((Y))                                      | Probability ((0 \\le P \\le 1))                                            |\n",
        "| **Equation**               | (Y = \\beta_0 + \\beta_1 X + \\varepsilon)                | (P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}})                      |\n",
        "| **Error Minimization**     | Least Squares (minimize squared differences)           | Maximum Likelihood Estimation (maximize probability of observed classes) |\n",
        "| **Linearity Assumption**   | Linear relationship between (X) and (Y)                | Linear relationship between (X) and **log-odds** of (Y)                  |\n",
        "| **Nature of Relationship** | Directly predicts value                                | Predicts probability of class; uses **sigmoid transformation**           |\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Intuition**\n",
        "\n",
        "* Linear regression can produce **predictions outside [0,1]**, which is not meaningful for probabilities.\n",
        "* Logistic regression **“squashes” predictions into [0,1]** using the sigmoid function.\n",
        "* Instead of modeling Y directly, logistic regression models the **log-odds (logit)**:\n",
        "\n",
        "[\n",
        "\\text{logit}(P) = \\log\\frac{P}{1-P} = \\beta_0 + \\beta_1 X\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Example**\n",
        "\n",
        "**Problem:** Predict whether a student passes (1) or fails (0) based on hours studied.\n",
        "\n",
        "* **Linear Regression Approach:** Might predict 1.2 or -0.3 → invalid as probability\n",
        "* **Logistic Regression Approach:** Predicts (P = 0.85) → interpret as 85% chance of passing, assign class 1\n",
        "\n",
        "---\n",
        "\n",
        "### **One-Line Summary**\n",
        "\n",
        "> **Logistic Regression predicts the probability of a categorical outcome using the logistic (sigmoid) function, unlike Linear Regression which predicts continuous values.**\n",
        "\n"
      ],
      "metadata": {
        "id": "8hNGYPTafB0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Name and briefly describe three common evaluation metrics for regression\n",
        "models.\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here are **three common evaluation metrics for regression models** with brief descriptions:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Mean Absolute Error (MAE)**\n",
        "\n",
        "[\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "]\n",
        "\n",
        "* Measures the **average absolute difference** between actual ((y_i)) and predicted ((\\hat{y}_i)) values.\n",
        "* **Interpretation:** On average, the predictions are off by **MAE units**.\n",
        "* **Advantage:** Simple, easy to understand.\n",
        "* **Disadvantage:** Does not penalize large errors more than small ones.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Mean Squared Error (MSE)**\n",
        "\n",
        "[\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "]\n",
        "\n",
        "* Measures the **average of squared differences** between actual and predicted values.\n",
        "* **Interpretation:** Larger errors are penalized more heavily due to squaring.\n",
        "* **Advantage:** Sensitive to large errors (useful if outliers are important).\n",
        "* **Disadvantage:** Squared units can make interpretation less intuitive.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. R-squared (Coefficient of Determination)**\n",
        "\n",
        "[\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}*i)^2}{\\sum*{i=1}^{n} (y_i - \\bar{y})^2}\n",
        "]\n",
        "\n",
        "* Measures the **proportion of variance in the dependent variable explained by the model**.\n",
        "* **Range:** 0 to 1 (sometimes negative if the model is worse than simply predicting the mean).\n",
        "* **Interpretation:**\n",
        "\n",
        "  * (R^2 = 0.8) → 80% of the variance in (Y) is explained by the model.\n",
        "* **Advantage:** Provides an intuitive measure of model fit.\n",
        "* **Disadvantage:** Can be misleading for non-linear relationships or overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Metric | What It Measures                 | Key Feature                 |\n",
        "| ------ | -------------------------------- | --------------------------- |\n",
        "| MAE    | Average absolute error           | Easy to interpret           |\n",
        "| MSE    | Average squared error            | Penalizes large errors more |\n",
        "| R²     | Proportion of variance explained | Indicates goodness of fit   |\n",
        "\n"
      ],
      "metadata": {
        "id": "G0aBVMuqfTW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a detailed explanation of the **purpose of the R-squared metric** in regression analysis:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Definition**\n",
        "\n",
        "**R-squared ((R^2))**, also called the **coefficient of determination**, measures the **proportion of variance in the dependent variable (Y) that is explained by the independent variable(s) (X)** in a regression model.\n",
        "\n",
        "[\n",
        "R^2 = 1 - \\frac{\\text{SS}*{\\text{res}}}{\\text{SS}*{\\text{tot}}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}*i)^2}{\\sum*{i=1}^{n} (y_i - \\bar{y})^2}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (y_i) = actual values\n",
        "* (\\hat{y}_i) = predicted values\n",
        "* (\\bar{y}) = mean of actual values\n",
        "* (\\text{SS}_{\\text{res}}) = sum of squared residuals\n",
        "* (\\text{SS}_{\\text{tot}}) = total sum of squares\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Purpose**\n",
        "\n",
        "1. **Assess Model Fit:**\n",
        "\n",
        "   * (R^2) indicates how well the regression line fits the data.\n",
        "   * High (R^2) → model explains most of the variance.\n",
        "   * Low (R^2) → model explains little of the variance.\n",
        "\n",
        "2. **Compare Models:**\n",
        "\n",
        "   * Useful for comparing different models predicting the same target.\n",
        "   * Higher (R^2) usually indicates a better fit (though beware of overfitting).\n",
        "\n",
        "3. **Interpretability:**\n",
        "\n",
        "   * Gives an **intuitive measure of how much of the variation in Y is captured by X**.\n",
        "   * Example: (R^2 = 0.85) → 85% of the variance in Y is explained by the model.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Important Notes**\n",
        "\n",
        "* (0 \\le R^2 \\le 1) for standard linear regression.\n",
        "* Negative (R^2) can occur if the model is **worse than predicting the mean**.\n",
        "* High (R^2) does **not guarantee causation** or that the model is correct.\n",
        "\n",
        "---\n",
        "\n",
        "### **Intuition**\n",
        "\n",
        "* Think of (R^2) as the **“goodness of fit” score**:\n",
        "\n",
        "  * 1 → perfect fit\n",
        "  * 0 → model explains nothing beyond the mean\n",
        "* It answers the question: **“How much of the variation in Y can be explained by X?”**\n",
        "\n"
      ],
      "metadata": {
        "id": "26lVcrwRfjaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "9. Write Python code to fit a simple linear regression model using scikit-learn\n",
        "and print the slope and intercept.\n",
        "\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print slope (coefficient) and intercept\n",
        "print(f\"Slope (beta_1): {model.coef_[0]:.4f}\")\n",
        "print(f\"Intercept (beta_0): {model.intercept_:.4f}\")\n"
      ],
      "metadata": {
        "id": "AQiYSBisf_rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of how to **interpret the coefficients in a Simple Linear Regression (SLR) model**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. SLR Equation Recap**\n",
        "\n",
        "The simple linear regression model is:\n",
        "\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* (Y) = dependent variable (target)\n",
        "* (X) = independent variable (feature)\n",
        "* (\\beta_0) = intercept\n",
        "* (\\beta_1) = slope\n",
        "* (\\varepsilon) = error term\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Intercept ((\\beta_0))**\n",
        "\n",
        "* **Definition:** Value of (Y) when (X = 0).\n",
        "* **Interpretation:** It is the **baseline value** of the target when the feature is zero.\n",
        "* **Example:**\n",
        "\n",
        "  * If (\\beta_0 = 50) in a house price model, it means a house with 0 square feet would start at $50,000 (or just the baseline in context).\n",
        "* ⚠️ Sometimes, (X = 0) may not be realistic, so interpretation should consider context.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Slope ((\\beta_1))**\n",
        "\n",
        "* **Definition:** Change in (Y) for a **one-unit increase** in (X).\n",
        "* **Interpretation:** Indicates the **direction and strength** of the relationship between (X) and (Y).\n",
        "\n",
        "  * Positive (\\beta_1): (Y) increases as (X) increases\n",
        "  * Negative (\\beta_1): (Y) decreases as (X) increases\n",
        "* **Example:**\n",
        "\n",
        "  * If (\\beta_1 = 3) in an advertising-sales model, every **$1 increase in advertising budget** increases sales by **$3** on average.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Intuition**\n",
        "\n",
        "* The **intercept** sets the starting point of the regression line.\n",
        "* The **slope** determines the **tilt** of the line (how steeply Y changes with X).\n",
        "* Together, they define the **best-fitting line** through the data that minimizes squared errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Coefficient           | Meaning                | Interpretation                          |\n",
        "| --------------------- | ---------------------- | --------------------------------------- |\n",
        "| Intercept ((\\beta_0)) | Value of Y when X=0    | Baseline level of the target            |\n",
        "| Slope ((\\beta_1))     | Change in Y per unit X | Direction and magnitude of relationship |\n",
        "\n"
      ],
      "metadata": {
        "id": "fDKjPoIHghz5"
      }
    }
  ]
}